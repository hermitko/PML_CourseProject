---
title: "Practical Machine Learning Course Project"
subtitle: "On the weightlifting exercise classification"
author: "Jan Herman"
date: "February 10, 2015"
output: html_document
bibliography: reference.bibtex
---

## Abstract 

In this analysis we aim to classify weightlifting exercises measurements
into 5 different classes. Each measurement contains data about one particular 
unilateral dumbbell biceps curl. The class variable corresponds to the way the 
exercise was done -- class *A* means correctly performed, classes *B*--*E* mean
different common mistakes. The dataset is publicly available from
[http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)
and described in detail in the original paper [@velloso2013qualitative].

The report consist of four parts -- at first we do a little exploratory analysis,
then comes the dataset cleaning part. In the third section we describe the
prediction model creation and estimate its performance. The last part summarizes
the results.

We have chosen predicting by random forests, since it is a powerfull and robust 
prediction method. Model accuracy estimated by the 10-fold cross validation is 99.5 %.

## Exploratory analysis

At first we load the R-libraries `dplyr`, `caret` and `ggplot2` used in the entire 
analysis [see @wickham_francois_2015, @topepo.github.io_2015 and @ggplot2].
Then we load  the dataset and determine the number of measurements 
and the number of variables.

```{r message=FALSE}
library(dplyr)
library(caret)
library(ggplot2)

pml <- read.csv("data/pml-training.csv")
dim(pml)
```

When we look at the summary of the dataset (see Apendix part), we see
that a lot of variables have vast majority of values either blank or NA.
Let us count those:
```{r}
sum(apply(pml, 2, function(x) mean(is.na(x) | x == "")) > .95)
```


So
`r sum(apply(pml, 2, function(x) mean(is.na(x) | x == "")) > .95)` 
out of total `r dim(pml)[2]` variables have more than 95 % blank 
or NA values.

As these variables carry little information, we discard them from
the further analysis.

## Data preparation
At first we remove the beforementioned variables with lots of blank values
together with another variables we consider of little use for classification
(the variables with timestamps, the ones describing the sliding windows and 
the variable `X` -- the measurement counter).

```{r message=FALSE}
pml_selected <- pml %>%
    select(which(
        apply(pml, 2, function(x) mean(is.na(x) | x == "")) <= .95)
        ) %>%
    select(-contains("timestamp"), -contains("window"), -X)
```

## Classification models
We have fitted two different classes of models. As a baseline we used one of the
simplest classification methods -- *k*-Nearest Neighbors [see @knn]. Then we 
tried more complicated models -- the Random Forest classificator 
[see @stat.berkeley.edu_2015]. Both kinds of models were constructed through
`caret` package wrapper [see @topepo.github.io_2015].

The accuracies of both classes of models are
evaluated by means of 10-fold cross validation.
Even though this is available as an automatized process in the `caret` package,
we decided to do it "by hand" to be seen clearly what is going on (and what is
under the hood of the `caret`).

Let us first divide the training dataset into 10 parts. We will train 10 classificators
of each type on 9 dataset parts and evaluate the accuracy on the remaining part.
The mean of these accuracies will be our 10-fold cross validation estimation of
the final model accuracy.

```{r cache=TRUE}
set.seed(213)

fold_count <- 10
folds <- createFolds(pml_selected$classe, k = fold_count)
```

### *k*-Nearest Neighbors
Now we train the *k*-NN models (with *k* = 10) ten times. Each time we train
it on the dataset with one particular fold removed. On this removed fold we 
later evaluate accuracy of the corresponding model. The final accuracy estimate
is the mean of these 10 accuracies.

```{r cache=TRUE}
models_knn <- lapply(1:fold_count,
                    function(i){
                        train(classe ~ .,
                        data = pml_selected[-folds[[i]], ],
                        method = "knn",
                        preProcess = c("center","scale"),
                        tuneGrid = data.frame(k = 10),
                        trControl = trainControl(method = "none"))
                    }
                    )                       

estimated_accuracies_knn <- 
    sapply(1:fold_count,
           function(i){
               mean(predict(models_knn[[i]], newdata = pml_selected[folds[[i]], ]) ==
                        pml_selected$classe[folds[[i]]])
           }
           )


print(mean(estimated_accuracies_knn))
```

Hence we have the baseline accuracy of approximately 
`r round(mean(estimated_accuracies_knn) * 100, 1)` % (pretty good, isnt't it?).
To build a little bit better model, we used Random Forests.

### Random Forests
As before, we train 10 models (each on 9 of 10 folds) and estimate the accuracy
on the missing fold.

```{r cache=TRUE, message=FALSE}
models_rf <- lapply(1:fold_count,
                    function(i){
                        train(classe ~ .,
                        data = pml_selected[-folds[[i]], ],
                        method = "rf", 
                        tuneGrid = data.frame(mtry = ncol(pml_selected) %/% 2),
                        trControl = trainControl(method = "none"))
                    }
                    )

estimated_accuracies_rf <- 
    sapply(1:fold_count,
           function(i){
               mean(predict(models_rf[[i]], newdata = pml_selected[folds[[i]], ]) ==
                        pml_selected$classe[folds[[i]]])
           }
           )

print(mean(estimated_accuracies_rf))
```

That means we achieved an accuracy estimate of (kind of unbelievable) 
`r round(mean(estimated_accuracies_rf) * 100, 1)` %!

Let us look on the accuracy in more detail. At first a figure of the confusion
matrix:
```{r message=FALSE}
predictions <- factor(rep_len(c("A", "B", "C", "D", "E"), nrow(pml_selected)))
for (i in 1:fold_count){
    predictions[folds[[i]]] <- 
        predict(models_rf[[i]], newdata = pml_selected[folds[[i]], ])
}

single_freq <- as.data.frame(table(pml_selected$classe))
frequencies <- as.data.frame(table(pml_selected$classe, predictions))
plot_data <- inner_join(single_freq, frequencies, by = c("Var1" = "Var1")) %>%
    mutate(Percentage = Freq.y / Freq.x)
ggplot(data = plot_data,
       aes(x = predictions, y = factor(Var1, levels(predictions)[5:1]), fill = Percentage)) +
    geom_tile() +
    scale_fill_gradient2(low = "white", high = "blue") +
    geom_text(aes(label = sprintf("%.3f %%", Percentage)), colour="black") +
    theme_bw() +
    theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank()) +
    scale_x_discrete(expand = c(0, 0)) + 
    scale_y_discrete(expand = c(0, 0)) + 
    labs(title = "Confusion matrix", y = "Actual class", 
         x = "Predicted class", fill = "Percentage") 

```

More detailed summary of the model accuracy can be seen in Appendix section.

## Conclusion

We have built *k*-NN and random forest prediction models that predict the way how the
exercise was done. The overall estimated accuracy of the random forest
predictor was `r round(mean(estimated_accuracies_rf) * 100, 1)` %.

## Appendix 

### Test data prediction

```{r}
pml_t <- read.csv("data/pml-testing.csv")

pml_selected_t <- pml_t %>%
    select(which(
        apply(pml, 2, function(x) mean(is.na(x) | x == "")) <= .95)
    ) %>%
    select(-contains("timestamp"), -contains("window"), -X)

predictions_t <- sapply(1:fold_count, 
                        function(i){
                            predict(models_rf[[i]], newdata = pml_selected_t)
                        }
                        )

prediction_t <- apply(predictions_t, 
                      1, 
                      function(x){names(which.max(table(x)))})
#prediction_t
```

Basic summary of the data (mentioned in the Exploratory analysis section):

```{r}
summary(pml)
```

Detailed model accuracy and other related statistics:

```{r}
confusionMatrix(predictions, pml_selected$classe)
```

## Notice
This report was done as a Course Project to Practical Machine Learning class on
[coursera.org](coursera.org) and is available on
[github](http://hermitko.github.io/PML_CourseProject/).

## References
