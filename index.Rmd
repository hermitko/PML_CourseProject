---
title: "Practical Machine Learning Course Project"
subtitle: "On the weightlifting exercise classification"
author: "Jan Herman"
date: "February 10, 2015"
output: html_document
bibliography: reference.bibtex
---

## Abstract 

In this analysis we aim to classify weightlifting exercises measurements
into 5 different classes. Each measurement contains data about one particular 
unilateral dumbbell biceps curl. The class variable corresponds to the way the 
exercise was done -- class *A* means correctly performed, classes *B*--*E* mean
different common mistakes. The dataset is publicly available from
[http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)
and described in detail in the original paper [@velloso2013qualitative].

The report consist of four parts -- at first we do a little exploratory analysis,
then comes the dataset cleaning part. In the third section we describe the
prediction model creation and estimate its performance. The last part summarizes
the results.

We have chosen predicting by random forests, since it is a powerfull and robust 
prediction method. Model accuracy estimated by the 10-fold cross validation is 99.5 %.

## Exploratory analysis

At first we load the R-libraries `dplyr`, `caret` and `ggplot2` used in the entire 
analysis [see @wickham_francois_2015, @topepo.github.io_2015 and @ggplot2].
Then we load  the dataset and determine the number of measurements 
and the number of variables.

```{r message=FALSE}
library(dplyr)
library(caret)
library(ggplot2)

pml <- read.csv("data/pml-training.csv")
dim(pml)
```

When we look at the summary of the dataset (see Apendix part), we see
that a lot of variables have vast majority of values either blank or NA.
Let us count those:
```{r}
sum(apply(pml, 2, function(x) mean(is.na(x) | x == "")) > .95)
```


So
`r sum(apply(pml, 2, function(x) mean(is.na(x) | x == "")) > .95)` 
out of total `r dim(pml)[2]` variables have more than 95 % blank 
or NA values.

As these variables carry little information, we discard them from
the further analysis.

## Data preparation
At first we remove the beforementioned variables with lots of blank values
together with another variables we consider of little use for classification
(the variables with timestamps, the ones describing the sliding windows and 
the variable `X` -- the measurement counter).

```{r message=FALSE}
pml_selected <- pml %>%
    select(which(
        apply(pml, 2, function(x) mean(is.na(x) | x == "")) <= .95)
        ) %>%
    select(-contains("timestamp"), -contains("window"), -X)
```

## Classification models
We have fitted two different model -- as a baseline, we used one of the
simplest classification methods -- *k*-Nearest Neighbors. Then we tried more
complicated model -- the Random Forest classificator 
[see @stat.berkeley.edu_2015]. Both models were constructed through
`caret` package wrapper [see @topepo.github.io_2015].

The accuracies of both models are
evaluated by means of 5-fold cross validation.
Even though this is available as an automatized process in the `caret` package,
we decided to do it "by hand" to be seen clearly what is going on (and what is
under the hood of the `caret`).

Let us first divide the training dataset into 5 parts. We will train 5 classificator
of each type on each 4 parts and evaluate the accuracy on the remaining part.
The mean of these accuracies will be our 5-fold cross validation estimation of
the final models accuracy.

```{r cache=TRUE}
set.seed(213)

fold_count <- 5
folds <- createFolds(pml_selected$classe, k = fold_count)
```

### *k*-Nearest Neighbors
Now we train the *k*-NN models (with *k* = 10) five times. Each time we train
it on the dataset with one particular fold removed. On this removed fold we 
later evaluate accuracy of the corresponding model. The final accuracy estimate
is the mean of these 5 accuracies.

```{r cache=TRUE}
models_knn <- lapply(1:fold_count,
                    function(i){
                        train(classe ~ .,
                        data = pml_selected[-folds[[i]], ],
                        method = "knn",
                        preProcess = c("center","scale"),
                        tuneGrid = data.frame(k = 10),
                        trControl = trainControl(method = "none"))
                    }
                    )                       

estimated_accuracies_knn <- 
    sapply(1:fold_count,
           function(i){
               mean(predict(models_knn[[i]], newdata = pml_selected[folds[[i]], ]) ==
                        pml_selected$classe[folds[[i]]])
           }
           )


print(mean(estimated_accuracies_knn))
```

Hence we have the baseline accuracy of approximately 
`r round(estimated_accuracy * 100, 1)` % (pretty good, isnt't it?). To build a 
little bit better model, we used Random Forests.

### Random Forests
As before, we train 5 models (each on 4 of 5 folds) and estimate the accuracy
on the missing fold.

```{r cache=TRUE}
models_rf <- lapply(1:fold_count,
                    function(i){
                        train(classe ~ .,
                        data = pml_selected[-folds[[i]], ],
                        method = "rf", 
                        trControl = trainControl(method = "none"))
                    }
                    )

estimated_accuracies_rf <- 
    sapply(1:fold_count,
           function(i){
               mean(predict(models_rf[[i]], newdata = pml_selected[folds[[i]], ]) ==
                        pml_selected$classe[folds[[i]]])
           }
           )

print(mean(estimated_accuracies_rf))
```

Let us look on the accuracy of the model. At first a figure of the confusion
matrix:
```{r}
single_freq <- as.data.frame(table(rf_model$pred$obs))
frequencies <- as.data.frame(table(rf_model$pred$obs, rf_model$pred$pred))
plot_data <- inner_join(single_freq, frequencies, by = c("Var1" = "Var1")) %>%
    mutate(Percentage = Freq.y / Freq.x)
ggplot(data = plot_data,
       aes(x = Var2, y = factor(Var1, levels(Var2)[5:1]), fill = Percentage)) +
    geom_tile() +
    scale_fill_gradient2(low = "white", high = "blue") +
    geom_text(aes(label = sprintf("%.3f %%", Percentage)), colour="black") +
    theme_bw() +
    theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank()) +
    scale_x_discrete(expand = c(0, 0)) + 
    scale_y_discrete(expand = c(0, 0)) + 
    labs(title = "Confusion matrix", y = "Actual class", 
         x = "Predicted class", fill = "Percentage") 

```

Looking at overall accuracy:

```{r}
mean(rf_model$pred$obs == rf_model$pred$pred)
```

We see, that our prediction model has an accuracy of 
`r round(mean(rf_model$pred$obs == rf_model$pred$pred) * 100, 1)` %. More 
detailed summary of the model accuracy can be seen in Appendix section.

## Conclusion

We have built random forest prediction model that predicts the way how the
exercise was done. The overall accuracy of the predictor was 
`r round(mean(rf_model$pred$obs == rf_model$pred$pred) * 100, 1)` %.

## Appendix 

Basic summary of the data (mentioned in the Exploratory analysis section):

```{r}
summary(pml)
```

Detailed model accuracy and other related statistics:

```{r}
confusionMatrix(rf_model$pred$pred, rf_model$pred$obs)
```

## Notice
This report was done as a Course Project to Practical Machine Learning class on
[coursera.org](coursera.org) and is available on
[github](http://hermitko.github.io/PML_CourseProject/).

## References
